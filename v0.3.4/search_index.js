var documenterSearchIndex = {"docs":
[{"location":"model_transformations/#Model-Transformations","page":"Model Transformations","title":"Model Transformations","text":"","category":"section"},{"location":"model_transformations/","page":"Model Transformations","title":"Model Transformations","text":"POMDPModelTools contains several tools for transforming problems into other classes so that they can be used by different solvers.","category":"page"},{"location":"model_transformations/#Sparse-Tabular-MDPs-and-POMDPs","page":"Model Transformations","title":"Sparse Tabular MDPs and POMDPs","text":"","category":"section"},{"location":"model_transformations/","page":"Model Transformations","title":"Model Transformations","text":"The SparseTabularMDP and SparseTabularPOMDP represents discrete problems defined using the explicit interface. The transition and observation models are represented using sparse matrices. Solver writers can leverage these data structures to write efficient vectorized code. A problem writer can define its problem using the explicit interface and it can be automatically converted to a sparse tabular representation by calling the constructors SparseTabularMDP(::MDP) or SparseTabularPOMDP(::POMDP). See the following docs to know more about the matrix representation and how to access the fields of the SparseTabular objects:","category":"page"},{"location":"model_transformations/","page":"Model Transformations","title":"Model Transformations","text":"SparseTabularMDP\nSparseTabularPOMDP\ntransition_matrix\nreward_vector\nobservation_matrix\ntransition_matrices\nreward_matrix\nobservation_matrices","category":"page"},{"location":"model_transformations/#POMDPModelTools.SparseTabularMDP","page":"Model Transformations","title":"POMDPModelTools.SparseTabularMDP","text":"SparseTabularMDP\n\nAn MDP object where states and actions are integers and the transition is represented by a list of sparse matrices. This data structure can be useful to exploit in vectorized algorithm (e.g. see SparseValueIterationSolver). The recommended way to access the transition and reward matrices is through the provided accessor functions: transition_matrix and reward_vector.\n\nFields\n\nT::Vector{SparseMatrixCSC{Float64, Int64}} The transition model is represented as a vector of sparse matrices (one for each action). T[a][s, sp] the probability of transition from s to sp taking action a.\nR::Array{Float64, 2} The reward is represented as a matrix where the rows are states and the columns actions: R[s, a] is the reward of taking action a in sate s.\nterminal_states::Set{Int64} Stores the terminal states\ndiscount::Float64 The discount factor\n\nConstructors\n\nSparseTabularMDP(mdp::MDP) : One can provide the matrices to the default constructor or one can construct a SparseTabularMDP from any discrete state MDP defined using the explicit interface. \n\nNote that constructing the transition and reward matrices requires to iterate over all the states and can take a while. To learn more information about how to define an MDP with the explicit interface please visit https://juliapomdp.github.io/POMDPs.jl/latest/explicit/ .\n\nSparseTabularMDP(smdp::SparseTabularMDP; transition, reward, discount) : This constructor returns a new sparse MDP that is a copy of the original smdp except for the field specified by the keyword arguments.\n\n\n\n\n\n","category":"type"},{"location":"model_transformations/#POMDPModelTools.SparseTabularPOMDP","page":"Model Transformations","title":"POMDPModelTools.SparseTabularPOMDP","text":"SparseTabularPOMDP\n\nA POMDP object where states and actions are integers and the transition and observation distributions are represented by lists of sparse matrices. This data structure can be useful to exploit in vectorized algorithms to gain performance (e.g. see SparseValueIterationSolver). The recommended way to access the transition, reward, and observation matrices is through the provided accessor functions: transition_matrix, reward_vector, observation_matrix.\n\nFields\n\nT::Vector{SparseMatrixCSC{Float64, Int64}} The transition model is represented as a vector of sparse matrices (one for each action). T[a][s, sp] the probability of transition from s to sp taking action a.\nR::Array{Float64, 2} The reward is represented as a matrix where the rows are states and the columns actions: R[s, a] is the reward of taking action a in sate s.\nO::Vector{SparseMatrixCSC{Float64, Int64}} The observation model is represented as a vector of sparse matrices (one for each action). O[a][sp, o] is the probability of observing o from state sp after having taken action a.\nterminal_states::Set{Int64} Stores the terminal states\ndiscount::Float64 The discount factor\n\nConstructors\n\nSparseTabularPOMDP(pomdp::POMDP) : One can provide the matrices to the default constructor or one can construct a SparseTabularPOMDP from any discrete state MDP defined using the explicit interface. \n\nNote that constructing the transition and reward matrices requires to iterate over all the states and can take a while. To learn more information about how to define an MDP with the explicit interface please visit https://juliapomdp.github.io/POMDPs.jl/latest/explicit/ .\n\nSparseTabularPOMDP(spomdp::SparseTabularMDP; transition, reward, observation, discount) : This constructor returns a new sparse POMDP that is a copy of the original smdp except for the field specified by the keyword arguments.\n\n\n\n\n\n","category":"type"},{"location":"model_transformations/#POMDPModelTools.transition_matrix","page":"Model Transformations","title":"POMDPModelTools.transition_matrix","text":"transition_matrix(p::SparseTabularProblem, a)\n\nAccessor function for the transition model of a sparse tabular problem. It returns a sparse matrix containing the transition probabilities when taking action a: T[s, sp] = Pr(sp | s, a).\n\n\n\n\n\n","category":"function"},{"location":"model_transformations/#POMDPModelTools.reward_vector","page":"Model Transformations","title":"POMDPModelTools.reward_vector","text":"reward_vector(p::SparseTabularProblem, a)\n\nAccessor function for the reward function of a sparse tabular problem. It returns a vector containing the reward for all the states when taking action a: R(s, a).  The length of the return vector is equal to the number of states.\n\n\n\n\n\n","category":"function"},{"location":"model_transformations/#POMDPModelTools.observation_matrix","page":"Model Transformations","title":"POMDPModelTools.observation_matrix","text":"observation_matrix(p::SparseTabularPOMDP, a::Int64)\n\nAccessor function for the observation model of a sparse tabular POMDP. It returns a sparse matrix containing the observation probabilities when having taken action a: O[sp, o] = Pr(o | sp, a).\n\n\n\n\n\n","category":"function"},{"location":"model_transformations/#POMDPModelTools.transition_matrices","page":"Model Transformations","title":"POMDPModelTools.transition_matrices","text":"transition_matrices(p::SparseTabularProblem)\n\nAccessor function for the transition model of a sparse tabular problem. It returns a list of sparse matrices for each action of the problem.\n\n\n\n\n\n","category":"function"},{"location":"model_transformations/#POMDPModelTools.reward_matrix","page":"Model Transformations","title":"POMDPModelTools.reward_matrix","text":"reward_matrix(p::SparseTabularProblem)\n\nAccessor function for the reward matrix R[s, a] of a sparse tabular problem.\n\n\n\n\n\n","category":"function"},{"location":"model_transformations/#POMDPModelTools.observation_matrices","page":"Model Transformations","title":"POMDPModelTools.observation_matrices","text":"observation_matrices(p::SparseTabularPOMDP)\n\nAccessor function for the observation model of a sparse tabular POMDP. It returns a list of sparse matrices for each action of the problem.\n\n\n\n\n\n","category":"function"},{"location":"model_transformations/#Fully-Observable-POMDP","page":"Model Transformations","title":"Fully Observable POMDP","text":"","category":"section"},{"location":"model_transformations/","page":"Model Transformations","title":"Model Transformations","text":"FullyObservablePOMDP","category":"page"},{"location":"model_transformations/#POMDPModelTools.FullyObservablePOMDP","page":"Model Transformations","title":"POMDPModelTools.FullyObservablePOMDP","text":"FullyObservablePOMDP(mdp)\n\nTurn MDP mdp into a POMDP where the observations are the states of the MDP.\n\n\n\n\n\n","category":"type"},{"location":"model_transformations/#Generative-Belief-MDP","page":"Model Transformations","title":"Generative Belief MDP","text":"","category":"section"},{"location":"model_transformations/","page":"Model Transformations","title":"Model Transformations","text":"Every POMDP is an MDP on the belief space GenerativeBeliefMDP creates a generative model for that MDP.","category":"page"},{"location":"model_transformations/","page":"Model Transformations","title":"Model Transformations","text":"warning: Warning\nThe reward generated by the GenerativeBeliefMDP is the reward for a single state sampled from the belief; it is not the   expected reward for that belief transition (though, in expectation, they are equivalent of course). Implementing the model with the expected reward requires a custom implementation because belief updaters do not typically deal with reward.","category":"page"},{"location":"model_transformations/","page":"Model Transformations","title":"Model Transformations","text":"GenerativeBeliefMDP","category":"page"},{"location":"model_transformations/#POMDPModelTools.GenerativeBeliefMDP","page":"Model Transformations","title":"POMDPModelTools.GenerativeBeliefMDP","text":"GenerativeBeliefMDP(pomdp, updater)\n\nCreate a generative model of the belief MDP corresponding to POMDP pomdp with belief updates performed by updater.\n\n\n\n\n\n","category":"type"},{"location":"model_transformations/#Example","page":"Model Transformations","title":"Example","text":"","category":"section"},{"location":"model_transformations/","page":"Model Transformations","title":"Model Transformations","text":"DocTestSetup = quote\n    using Pkg\n    Pkg.add(\"POMDPModels\")\n    Pkg.add(\"BeliefUpdaters\")\n    # the thing below should be a jldoctest once POMDPSimulators gets registered\nend","category":"page"},{"location":"model_transformations/","page":"Model Transformations","title":"Model Transformations","text":"using POMDPModels\nusing POMDPModelTools\nusing BeliefUpdaters\n\npomdp = BabyPOMDP()\nupdater = DiscreteUpdater(pomdp)\n\nbelief_mdp = GenerativeBeliefMDP(pomdp, updater)\n@show statetype(belief_mdp) # POMDPModels.BoolDistribution\n\nfor (a, r, sp) in stepthrough(belief_mdp, RandomPolicy(belief_mdp), \"a,r,sp\", max_steps=5)\n    @show a, r, sp\nend","category":"page"},{"location":"model_transformations/","page":"Model Transformations","title":"Model Transformations","text":"DocTestSetup = nothing","category":"page"},{"location":"model_transformations/#Underlying-MDP","page":"Model Transformations","title":"Underlying MDP","text":"","category":"section"},{"location":"model_transformations/","page":"Model Transformations","title":"Model Transformations","text":"UnderlyingMDP","category":"page"},{"location":"model_transformations/#POMDPModelTools.UnderlyingMDP","page":"Model Transformations","title":"POMDPModelTools.UnderlyingMDP","text":"UnderlyingMDP(m::POMDP)\n\nTransform POMDP m into an MDP where the states are fully observed.\n\nUnderlyingMDP(m::MDP)\n\nReturn m\n\n\n\n\n\n","category":"type"},{"location":"model_transformations/#State-Action-Reward-Model","page":"Model Transformations","title":"State Action Reward Model","text":"","category":"section"},{"location":"model_transformations/","page":"Model Transformations","title":"Model Transformations","text":"StateActionReward","category":"page"},{"location":"model_transformations/#POMDPModelTools.StateActionReward","page":"Model Transformations","title":"POMDPModelTools.StateActionReward","text":"StateActionReward(m::Union{MDP,POMDP})\n\nRobustly create a reward function that depends only on the state and action.\n\nIf reward(m, s, a) is implemented, that will be used, otherwise the mean of reward(m, s, a, sp) for MDPs or reward(m, s, a, sp, o) for POMDPs will be used.\n\nExample\n\nusing POMDPs\nusing POMDPModels\nusing POMDPModelTools\n\nm = BabyPOMDP()\n\nrm = StateActionReward(m)\n\nrm(true, true)\n\n# output\n\n-15.0\n\n\n\n\n\n","category":"type"},{"location":"policy_evaluation/#Policy-Evaluation","page":"Policy Evaluation","title":"Policy Evaluation","text":"","category":"section"},{"location":"policy_evaluation/","page":"Policy Evaluation","title":"Policy Evaluation","text":"The evaluate function provides a policy evaluation tool for MDPs:","category":"page"},{"location":"policy_evaluation/","page":"Policy Evaluation","title":"Policy Evaluation","text":"evaluate","category":"page"},{"location":"policy_evaluation/#POMDPModelTools.evaluate","page":"Policy Evaluation","title":"POMDPModelTools.evaluate","text":"evaluate(m::MDP, p::Policy)\nevaluate(m::MDP, p::Policy; rewardfunction=POMDPs.reward)\n\nCalculate the value for a policy on an MDP using the approach in equation 4.2.2 of Kochenderfer, Decision Making Under Uncertainty, 2015.\n\nReturns a DiscreteValueFunction, which maps states to values.\n\nExample\n\nusing POMDPModelTools, POMDPPolicies, POMDPModels\nm = SimpleGridWorld()\nu = evaluate(m, FunctionPolicy(x->:left))\nu([1,1]) # value of always moving left starting at state [1,1]\n\n\n\n\n\n","category":"function"},{"location":"convenience/#Convenience","page":"Convenience","title":"Convenience","text":"","category":"section"},{"location":"convenience/","page":"Convenience","title":"Convenience","text":"POMDPModelTools contains default implementations for some POMDPs.jl functions.","category":"page"},{"location":"convenience/","page":"Convenience","title":"Convenience","text":"For a complete list of default implementations, see convenient_implementations.jl.","category":"page"},{"location":"distributions/#Distributions","page":"Distributions","title":"Distributions","text":"","category":"section"},{"location":"distributions/","page":"Distributions","title":"Distributions","text":"POMDPModelTools contains several utility distributions to be used in the POMDPs transition and observation functions. These implement the appropriate methods of the functions in the distributions interface.","category":"page"},{"location":"distributions/","page":"Distributions","title":"Distributions","text":"This package also supplies showdistribution for pretty printing distributions as unicode bar graphs to the terminal.","category":"page"},{"location":"distributions/#Sparse-Categorical-(SparseCat)","page":"Distributions","title":"Sparse Categorical (SparseCat)","text":"","category":"section"},{"location":"distributions/","page":"Distributions","title":"Distributions","text":"SparseCat is a sparse categorical distribution which is specified by simply providing a list of possible values (states or observations) and the probabilities corresponding to those particular objects.","category":"page"},{"location":"distributions/","page":"Distributions","title":"Distributions","text":"Example: SparseCat([1,2,3], [0.1,0.2,0.7]) is a categorical distribution that assigns probability 0.1 to 1, 0.2 to 2, 0.7 to 3, and 0 to all other values.","category":"page"},{"location":"distributions/","page":"Distributions","title":"Distributions","text":"SparseCat","category":"page"},{"location":"distributions/#POMDPModelTools.SparseCat","page":"Distributions","title":"POMDPModelTools.SparseCat","text":"SparseCat(values, probabilities)\n\nCreate a sparse categorical distribution.\n\nvalues is an iterable object containing the possible values (can be of any type) in the distribution that have nonzero probability. probabilities is an iterable object that contains the associated probabilities.\n\nThis is optimized for value iteration with a fast implementation of weighted_iterator. Both pdf and rand are order n.\n\n\n\n\n\n","category":"type"},{"location":"distributions/#Implicit","page":"Distributions","title":"Implicit","text":"","category":"section"},{"location":"distributions/","page":"Distributions","title":"Distributions","text":"In situations where a distribution object is required, but the pdf is difficult to specify and only samples are required, ImplicitDistribution provides a convenient way to package a sampling function.","category":"page"},{"location":"distributions/","page":"Distributions","title":"Distributions","text":"ImplicitDistribution","category":"page"},{"location":"distributions/#POMDPModelTools.ImplicitDistribution","page":"Distributions","title":"POMDPModelTools.ImplicitDistribution","text":"ImplicitDistribution(sample_function, args...)\n\nDefine a distribution that can only be sampled from using rand, but has no explicit pdf.\n\nEach time rand(rng, d::ImplicitDistribution) is called,\n\nsample_function(args..., rng)\n\nwill be called to generate a new sample.\n\nImplicitDistribution is designed to be used with anonymous functions or the do syntax as follows:\n\nExamples\n\nImplicitDistribution(rng->rand(rng)^2)\n\nstruct MyMDP <: MDP{Float64, Int} end\n\nfunction POMDPs.transition(m::MyMDP, s, a)\n    ImplicitDistribution(s, a) do s, a, rng\n        return s + a + 0.001*randn(rng)\n    end\nend\n\ntd = transition(MyMDP(), 1.0, 1)\nrand(td) # will return a number near 2\n\n\n\n\n\n","category":"type"},{"location":"distributions/#Bool-Distribution","page":"Distributions","title":"Bool Distribution","text":"","category":"section"},{"location":"distributions/","page":"Distributions","title":"Distributions","text":"BoolDistribution","category":"page"},{"location":"distributions/#POMDPModelTools.BoolDistribution","page":"Distributions","title":"POMDPModelTools.BoolDistribution","text":"BoolDistribution(p_true)\n\nCreate a distribution over Boolean values (true or false).\n\np_true is the probability of the true outcome; the probability of false is 1-p_true.\n\n\n\n\n\n","category":"type"},{"location":"distributions/#Deterministic","page":"Distributions","title":"Deterministic","text":"","category":"section"},{"location":"distributions/","page":"Distributions","title":"Distributions","text":"Deterministic","category":"page"},{"location":"distributions/#POMDPModelTools.Deterministic","page":"Distributions","title":"POMDPModelTools.Deterministic","text":"Deterministic(value)\n\nCreate a deterministic distribution over only one value.\n\nThis is intended to be used when a distribution is required, but the outcome is deterministic. It is equivalent to a Kronecker Delta distribution.\n\n\n\n\n\n","category":"type"},{"location":"distributions/#Uniform","page":"Distributions","title":"Uniform","text":"","category":"section"},{"location":"distributions/","page":"Distributions","title":"Distributions","text":"Uniform\nUnsafeUniform","category":"page"},{"location":"distributions/#POMDPModelTools.Uniform","page":"Distributions","title":"POMDPModelTools.Uniform","text":"Uniform(collection)\n\nCreate a uniform categorical distribution over a collection of objects.\n\nThe objects in the collection must be unique (this is tested on construction), and will be stored in a Set. To avoid this overhead, use UnsafeUniform.\n\n\n\n\n\n","category":"type"},{"location":"distributions/#POMDPModelTools.UnsafeUniform","page":"Distributions","title":"POMDPModelTools.UnsafeUniform","text":"UnsafeUniform(collection)\n\nCreate a uniform categorical distribution over a collection of objects.\n\nNo checks are performed to ensure uniqueness or check whether an object is actually in the set when evaluating the pdf.\n\n\n\n\n\n","category":"type"},{"location":"distributions/#Pretty-Printing","page":"Distributions","title":"Pretty Printing","text":"","category":"section"},{"location":"distributions/","page":"Distributions","title":"Distributions","text":"showdistribution","category":"page"},{"location":"distributions/#POMDPModelTools.showdistribution","page":"Distributions","title":"POMDPModelTools.showdistribution","text":"showdistribution([io], [mime], d)\n\nShow a UnicodePlots.barplot representation of a distribution.\n\nKeyword Arguments\n\ntitle::String=string(typeof(d))*\" distribution\": title for the barplot. \n\n\n\n\n\n","category":"function"},{"location":"interface_extensions/#Interface-Extensions","page":"Interface Extensions","title":"Interface Extensions","text":"","category":"section"},{"location":"interface_extensions/","page":"Interface Extensions","title":"Interface Extensions","text":"POMDPModelTools contains several interface extensions that provide shortcuts and standardized ways of dealing with extra data.","category":"page"},{"location":"interface_extensions/","page":"Interface Extensions","title":"Interface Extensions","text":"Programmers should use these functions whenever possible in case optimized implementations are available, but all of the functions have default implementations based on the core POMDPs.jl interface. Thus, if the core interface is implemented, all of these functions will also be available.","category":"page"},{"location":"interface_extensions/#Weighted-Iteration","page":"Interface Extensions","title":"Weighted Iteration","text":"","category":"section"},{"location":"interface_extensions/","page":"Interface Extensions","title":"Interface Extensions","text":"Many solution techniques, for example value iteration, require iteration through the support of a distribution and evaluating the probability mass for each value. In some cases, looking up the probability mass is expensive, so it is more efficient to iterate through value => probability pairs. weighted_iterator provides a standard interface for this.","category":"page"},{"location":"interface_extensions/","page":"Interface Extensions","title":"Interface Extensions","text":"weighted_iterator","category":"page"},{"location":"interface_extensions/#POMDPModelTools.weighted_iterator","page":"Interface Extensions","title":"POMDPModelTools.weighted_iterator","text":"weighted_iterator(d)\n\nReturn an iterator through pairs of the values and probabilities in distribution d.\n\nThis is designed to speed up value iteration. Distributions are encouraged to provide a custom optimized implementation if possible.\n\nExample\n\njulia> d = BoolDistribution(0.7)\nBoolDistribution(0.7)\n\njulia> collect(weighted_iterator(d))\n2-element Array{Pair{Bool,Float64},1}:\n  true => 0.7\n false => 0.3\n\n\n\n\n\n","category":"function"},{"location":"interface_extensions/#Observation-Weight","page":"Interface Extensions","title":"Observation Weight","text":"","category":"section"},{"location":"interface_extensions/","page":"Interface Extensions","title":"Interface Extensions","text":"Sometimes, e.g. in particle filtering, the relative likelihood of an observation is required in addition to a generative model, and it is often tedious to implement a custom observation distribution type. For this case, the shortcut function obs_weight is provided.","category":"page"},{"location":"interface_extensions/","page":"Interface Extensions","title":"Interface Extensions","text":"obs_weight","category":"page"},{"location":"interface_extensions/#POMDPModelTools.obs_weight","page":"Interface Extensions","title":"POMDPModelTools.obs_weight","text":"obs_weight(pomdp, s, a, sp, o)\n\nReturn a weight proportional to the likelihood of receiving observation o from state sp (and a and s if they are present).\n\nThis is a useful shortcut for particle filtering so that the observation distribution does not have to be represented.\n\n\n\n\n\n","category":"function"},{"location":"interface_extensions/#Ordered-Spaces","page":"Interface Extensions","title":"Ordered Spaces","text":"","category":"section"},{"location":"interface_extensions/","page":"Interface Extensions","title":"Interface Extensions","text":"It is often useful to have a list of states, actions, or observations ordered consistently with the respective _index function from POMDPs.jl. Since the POMDPs.jl interface does not demand that spaces be ordered consistently with _index, the states, actions, and observations functions are not sufficient. Thus POMDPModelTools provides ordered_actions, ordered_states, and ordered_observations to provide this capability.","category":"page"},{"location":"interface_extensions/","page":"Interface Extensions","title":"Interface Extensions","text":"ordered_actions\nordered_states\nordered_observations","category":"page"},{"location":"interface_extensions/#POMDPModelTools.ordered_actions","page":"Interface Extensions","title":"POMDPModelTools.ordered_actions","text":"ordered_actions(mdp)\n\nReturn an AbstractVector of actions ordered according to actionindex(mdp, a).\n\nordered_actions(mdp) will always return an AbstractVector{A} v containing all of the actions in actions(mdp) in the order such that actionindex(mdp, v[i]) == i. You may wish to override this for your problem for efficiency.\n\n\n\n\n\n","category":"function"},{"location":"interface_extensions/#POMDPModelTools.ordered_states","page":"Interface Extensions","title":"POMDPModelTools.ordered_states","text":"ordered_states(mdp)\n\nReturn an AbstractVector of states ordered according to stateindex(mdp, a).\n\nordered_states(mdp) will always return a AbstractVector{A} v containing all of the states in states(mdp) in the order such that stateindex(mdp, v[i]) == i. You may wish to override this for your problem for efficiency.\n\n\n\n\n\n","category":"function"},{"location":"interface_extensions/#POMDPModelTools.ordered_observations","page":"Interface Extensions","title":"POMDPModelTools.ordered_observations","text":"ordered_observations(pomdp)\n\nReturn an AbstractVector of observations ordered according to obsindex(pomdp, a).\n\nordered_observations(mdp) will always return a AbstractVector{A} v containing all of the observations in observations(pomdp) in the order such that obsindex(pomdp, v[i]) == i. You may wish to override this for your problem for efficiency.\n\n\n\n\n\n","category":"function"},{"location":"interface_extensions/#Info-Interface","page":"Interface Extensions","title":"Info Interface","text":"","category":"section"},{"location":"interface_extensions/","page":"Interface Extensions","title":"Interface Extensions","text":"It is often the case that useful information besides the belief, state, action, etc is generated by a function in POMDPs.jl. This information can be useful for debugging or understanding the behavior of a solver, updater, or problem. The info interface provides a standard way for problems, policies, solvers or updaters to output this information. The recording simulators from POMDPSimulators.jl automatically record this information.","category":"page"},{"location":"interface_extensions/","page":"Interface Extensions","title":"Interface Extensions","text":"To specify info for a problem (in POMDPs v0.8 and above), one should modify the problem's DDN with the add_infonode function, then return the info in gen. There is an example of this pattern in the docstring below:","category":"page"},{"location":"interface_extensions/","page":"Interface Extensions","title":"Interface Extensions","text":"add_infonode","category":"page"},{"location":"interface_extensions/","page":"Interface Extensions","title":"Interface Extensions","text":"To specify info from policies, solvers, or updaters, implement the following functions:","category":"page"},{"location":"interface_extensions/","page":"Interface Extensions","title":"Interface Extensions","text":"action_info\nsolve_info\nupdate_info","category":"page"},{"location":"interface_extensions/#POMDPModelTools.action_info","page":"Interface Extensions","title":"POMDPModelTools.action_info","text":"a, ai = action_info(policy, x)\n\nReturn a tuple containing the action determined by policy 'p' at state or belief 'x' and information (usually a NamedTuple, Dict or nothing) from the calculation of that action.\n\nBy default, returns nothing as info.\n\n\n\n\n\n","category":"function"},{"location":"interface_extensions/#POMDPModelTools.solve_info","page":"Interface Extensions","title":"POMDPModelTools.solve_info","text":"policy, si = solve_info(solver, problem)\n\nReturn a tuple containing the policy determined by a solver and information (usually a NamedTuple, Dict or nothing) from the calculation of that policy.\n\nBy default, returns nothing as info.\n\n\n\n\n\n","category":"function"},{"location":"interface_extensions/#POMDPModelTools.update_info","page":"Interface Extensions","title":"POMDPModelTools.update_info","text":"bp, i = update_info(updater, b, a, o)\n\nReturn a tuple containing the new belief and information (usually a NamedTuple, Dict or nothing) from the belief update.\n\nBy default, returns nothing as info.\n\n\n\n\n\n","category":"function"},{"location":"common_rl/#CommonRLInterface-Integration","page":"CommonRLInterface Integration","title":"CommonRLInterface Integration","text":"","category":"section"},{"location":"common_rl/","page":"CommonRLInterface Integration","title":"CommonRLInterface Integration","text":"The POMDPModelTools provides two-way integration with the CommonRLInterface.jl package. Using the convert function, one can convert an MDP or POMDP object to a CommonRLInterface environment, or vice-versa.","category":"page"},{"location":"common_rl/","page":"CommonRLInterface Integration","title":"CommonRLInterface Integration","text":"For example,","category":"page"},{"location":"common_rl/","page":"CommonRLInterface Integration","title":"CommonRLInterface Integration","text":"using POMDPs\nusing POMDPModelTools\nusing POMDPModels\nusing CommonRLInterface\n\nenv = convert(AbstractEnv, BabyPOMDP())\n\nr = act!(env, true)\nobserve(env)","category":"page"},{"location":"common_rl/","page":"CommonRLInterface Integration","title":"CommonRLInterface Integration","text":"converts a Crying Baby POMDP to an RL environment and acts in and observes the environment. This environment (or any other CommonRLInterface environment), can be converted to an MDP or POMDP:","category":"page"},{"location":"common_rl/","page":"CommonRLInterface Integration","title":"CommonRLInterface Integration","text":"using BasicPOMCP\n\nm = convert(POMDP, env)\nplanner = solve(POMCPSolver(), m)\na = action(planner, initialstate(m))","category":"page"},{"location":"common_rl/","page":"CommonRLInterface Integration","title":"CommonRLInterface Integration","text":"You can also use the constructors listed below to manually convert between the interfaces.","category":"page"},{"location":"common_rl/#Environment-Wrapper-Types","page":"CommonRLInterface Integration","title":"Environment Wrapper Types","text":"","category":"section"},{"location":"common_rl/","page":"CommonRLInterface Integration","title":"CommonRLInterface Integration","text":"Since the standard reinforcement learning environment interface offers less information about the internal workings of the environment than the POMDPs.jl interface, MDPs and POMDPs created from these environments will have limited functionality. There are two types of (PO)MDP types that can wrap an environment:","category":"page"},{"location":"common_rl/#Generative-model-wrappers","page":"CommonRLInterface Integration","title":"Generative model wrappers","text":"","category":"section"},{"location":"common_rl/","page":"CommonRLInterface Integration","title":"CommonRLInterface Integration","text":"If the state and setstate! CommonRLInterface functions are provided, then the environment can be wrapped in a RLEnvMDP or RLEnvPOMDP and the POMDPs.jl generative model interface will be available.","category":"page"},{"location":"common_rl/#Opaque-wrappers","page":"CommonRLInterface Integration","title":"Opaque wrappers","text":"","category":"section"},{"location":"common_rl/","page":"CommonRLInterface Integration","title":"CommonRLInterface Integration","text":"If the state and setstate! are not provided, then the resulting POMDP or MDP can only be simulated. This case is represented using the OpaqueRLEnvPOMDP and OpaqueRLEnvMDP wrappers. From the POMDPs.jl perspective, the state of the opaque (PO)MDP is just an integer wrapped in an OpaqueRLEnvState. This keeps track of the \"age\" of the environment so that POMDPs.jl actions that attempt to interact with the environment at a different age are invalid.","category":"page"},{"location":"common_rl/#Constructors","page":"CommonRLInterface Integration","title":"Constructors","text":"","category":"section"},{"location":"common_rl/#Creating-RL-environments-from-MDPs-and-POMDPs","page":"CommonRLInterface Integration","title":"Creating RL environments from MDPs and POMDPs","text":"","category":"section"},{"location":"common_rl/","page":"CommonRLInterface Integration","title":"CommonRLInterface Integration","text":"MDPCommonRLEnv\nPOMDPCommonRLEnv","category":"page"},{"location":"common_rl/#POMDPModelTools.MDPCommonRLEnv","page":"CommonRLInterface Integration","title":"POMDPModelTools.MDPCommonRLEnv","text":"MDPCommonRLEnv(m, [s])\nMDPCommonRLEnv{RLO}(m, [s])\n\nCreate a CommonRLInterface environment from MDP m; optionally specify the state 's'.\n\nThe RLO parameter can be used to specify a type to convert the observation to. By default, this is AbstractArray. Use Any to disable conversion.\n\n\n\n\n\n","category":"type"},{"location":"common_rl/#POMDPModelTools.POMDPCommonRLEnv","page":"CommonRLInterface Integration","title":"POMDPModelTools.POMDPCommonRLEnv","text":"POMDPCommonRLEnv(m, [s], [o])\nPOMDPCommonRLEnv{RLO}(m, [s], [o])\n\nCreate a CommonRLInterface environment from POMDP m; optionally specify the state 's' and observation 'o'.\n\nThe RLO parameter can be used to specify a type to convert the observation to. By default, this is AbstractArray. Use Any to disable conversion.\n\n\n\n\n\n","category":"type"},{"location":"common_rl/#Creating-MDPs-and-POMDPs-from-RL-environments","page":"CommonRLInterface Integration","title":"Creating MDPs and POMDPs from RL environments","text":"","category":"section"},{"location":"common_rl/","page":"CommonRLInterface Integration","title":"CommonRLInterface Integration","text":"RLEnvMDP\nRLEnvPOMDP\nOpaqueRLEnvMDP\nOpaqueRLEnvPOMDP","category":"page"},{"location":"common_rl/#POMDPModelTools.RLEnvMDP","page":"CommonRLInterface Integration","title":"POMDPModelTools.RLEnvMDP","text":"RLEnvMDP(env; discount=1.0)\n\nCreate an MDP by wrapping a CommonRLInterface.AbstractEnv. state and setstate! from CommonRLInterface must be provided, and the POMDPs generative model functionality will be provided.\n\n\n\n\n\n","category":"type"},{"location":"common_rl/#POMDPModelTools.RLEnvPOMDP","page":"CommonRLInterface Integration","title":"POMDPModelTools.RLEnvPOMDP","text":"RLEnvPOMDP(env; discount=1.0)\n\nCreate an POMDP by wrapping a CommonRLInterface.AbstractEnv. state and setstate! from CommonRLInterface must be provided, and the POMDPs generative model functionality will be provided.\n\n\n\n\n\n","category":"type"},{"location":"common_rl/#POMDPModelTools.OpaqueRLEnvMDP","page":"CommonRLInterface Integration","title":"POMDPModelTools.OpaqueRLEnvMDP","text":"OpaqueRLEnvMDP(env; discount=1.0)\n\nWrap a CommonRLInterface.AbstractEnv in an MDP object. The state will be an OpaqueRLEnvState and only simulation will be supported.\n\n\n\n\n\n","category":"type"},{"location":"common_rl/#POMDPModelTools.OpaqueRLEnvPOMDP","page":"CommonRLInterface Integration","title":"POMDPModelTools.OpaqueRLEnvPOMDP","text":"OpaqueRLEnvPOMDP(env; discount=1.0)\n\nWrap a CommonRLInterface.AbstractEnv in an POMDP object. The state will be an OpaqueRLEnvState and only simulation will be supported.\n\n\n\n\n\n","category":"type"},{"location":"visualization/#Visualization","page":"Visualization","title":"Visualization","text":"","category":"section"},{"location":"visualization/","page":"Visualization","title":"Visualization","text":"POMDPModelTools contains a basic visualization interface consisting of the render function.","category":"page"},{"location":"visualization/","page":"Visualization","title":"Visualization","text":"Problem writers should implement a method of this function so that their problem can be visualized in a variety of contexts including jupyter notebooks, web browsers, or saved as images or animations.","category":"page"},{"location":"visualization/","page":"Visualization","title":"Visualization","text":"render","category":"page"},{"location":"visualization/#POMDPModelTools.render","page":"Visualization","title":"POMDPModelTools.render","text":"render(m::Union{MDP,POMDP}, step::NamedTuple)\n\nReturn a renderable representation of the step in problem m.\n\nThe renderable representation may be anything that has show(io, mime, x) methods. It could be a plot, svg, Compose.jl context, Cairo context, or image.\n\nArguments\n\nstep is a NamedTuple that contains the states, action, etc. corresponding to one transition in a simulation. It may have the following fields:\n\nt: the time step index\ns: the state at the beginning of the step\na: the action\nsp: the state at the end of the step (s')\nr: the reward for the step\no: the observation\nb: the belief at the \nbp: the belief at the end of the step\ni: info from the model when the state transition was calculated\nai: info from the policy decision\nui: info from the belief update\n\nKeyword arguments are reserved for the problem implementer and can be used to control appearance, etc.\n\nImportant Notes\n\nstep may not contain all of the elements listed above, so render should check for them and render only what is available\no typically corresponds to sp, so it is often clearer for POMDPs to render sp rather than s.\n\n\n\n\n\n","category":"function"},{"location":"visualization/","page":"Visualization","title":"Visualization","text":"Sometimes it is important to have control over how the problem is rendered with different mimetypes. One way to handle this is to have render return a custom type, e.g.","category":"page"},{"location":"visualization/","page":"Visualization","title":"Visualization","text":"struct MyProblemVisualization\n    mdp::MyProblem\n    step::NamedTuple\nend\n\nPOMDPModelTools.render(mdp, step) = MyProblemVisualization(mdp, step)","category":"page"},{"location":"visualization/","page":"Visualization","title":"Visualization","text":"and then implement custom show methods, e.g.","category":"page"},{"location":"visualization/","page":"Visualization","title":"Visualization","text":"show(io::IO, mime::MIME\"text/html\", v::MyProblemVisualization)","category":"page"},{"location":"#Home","page":"Home","title":"Home","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"POMDPModelTools is a collection of interface extensions and tools to make writing models and solvers for POMDPs.jl easier.","category":"page"},{"location":"","page":"Home","title":"Home","text":"","category":"page"},{"location":"utility_types/#Utility-Types","page":"Utility Types","title":"Utility Types","text":"","category":"section"},{"location":"utility_types/#Terminal-State","page":"Utility Types","title":"Terminal State","text":"","category":"section"},{"location":"utility_types/","page":"Utility Types","title":"Utility Types","text":"TerminalState and its singleton instance terminalstate are available to use for a terminal state in concert with another state type. It has the appropriate type promotion logic to make its use with other types friendly, similar to nothing and missing.","category":"page"},{"location":"utility_types/","page":"Utility Types","title":"Utility Types","text":"note: Note\nNOTE: This is NOT a replacement for the standard POMDPs.jl isterminal function, though isterminal is implemented for the type. It is merely a convenient type to use for terminal states.","category":"page"},{"location":"utility_types/","page":"Utility Types","title":"Utility Types","text":"warning: Warning\nWARNING: Early tests (August 2018) suggest that the Julia 1.0 compiler will not be able to efficiently implement union splitting in cases as  complex as POMDPs, so using a Union for the state type of a problem can currently have a large overhead.","category":"page"},{"location":"utility_types/","page":"Utility Types","title":"Utility Types","text":"TerminalState\nterminalstate","category":"page"},{"location":"utility_types/#POMDPModelTools.TerminalState","page":"Utility Types","title":"POMDPModelTools.TerminalState","text":"TerminalState\n\nA type with no fields whose singleton instance terminalstate is used to represent a terminal state with no additional information.\n\nThis type has the appropriate promotion logic implemented to function like Missing when added to arrays, etc.\n\nNote that terminal states NEED NOT be of type TerminalState. You can define any state to be terminal by implementing the appropriate isterminal method. Solvers and simulators SHOULD NOT check for this type, but should instead check using isterminal. \n\n\n\n\n\n","category":"type"},{"location":"utility_types/#POMDPModelTools.terminalstate","page":"Utility Types","title":"POMDPModelTools.terminalstate","text":"terminalstate\n\nThe singleton instance of type TerminalState representing a terminal state.\n\n\n\n\n\n","category":"constant"}]
}
